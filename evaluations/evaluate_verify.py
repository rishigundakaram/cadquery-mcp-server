#!/usr/bin/env python3
"""
Evaluation harness for the CAD verification MCP tool.

This script tests the verify_cad_query function against a set of test models
with known expected results to measure verification accuracy.
"""

import json
import subprocess
import sys
from pathlib import Path
from typing import Any

# Import the server module
sys.path.append(str(Path(__file__).parent.parent))
import server


def load_test_cases() -> dict[str, dict[str, str]]:
    """Load test cases from folder structure."""
    test_cases_dir = Path(__file__).parent / "test_cases"
    if not test_cases_dir.exists():
        raise FileNotFoundError(f"Test cases directory not found: {test_cases_dir}")

    test_cases = {}
    for case_dir in test_cases_dir.iterdir():
        if not case_dir.is_dir():
            continue
            
        model_file = case_dir / "model.py"
        criteria_file = case_dir / "criteria.txt"
        result_file = case_dir / "result.txt"
        
        if all(f.exists() for f in [model_file, criteria_file, result_file]):
            test_cases[case_dir.name] = {
                "model_path": str(model_file),
                "criteria": criteria_file.read_text().strip(),
                "expected": result_file.read_text().strip()
            }
    
    return test_cases


def generate_visual_outputs(model_file: Path, output_dir: Path) -> dict[str, str]:
    """Generate SVG visual outputs for a model file."""
    output_dir.mkdir(parents=True, exist_ok=True)

    # Find the root project directory (where src/ is located)
    project_root = Path(__file__).parent.parent

    outputs = {}

    try:
        # Generate SVG views using the render_views script
        render_script = project_root / "src" / "ai_3d_print" / "render_views.py"
        if render_script.exists():
            result = subprocess.run(
                ["uv", "run", "python", str(render_script), str(model_file)],
                cwd=project_root,
                capture_output=True,
                text=True,
                timeout=60,
            )

            if result.returncode == 0:
                # Move generated SVG files to the output directory
                model_name = model_file.stem
                svg_patterns = [
                    f"{model_name}_top.svg",
                    f"{model_name}_front.svg",
                    f"{model_name}_right.svg",
                    f"{model_name}_iso.svg",
                ]

                for svg_file in svg_patterns:
                    src_path = project_root / "outputs" / svg_file
                    if src_path.exists():
                        dst_path = output_dir / svg_file
                        src_path.rename(dst_path)
                        outputs[svg_file] = str(dst_path)

                outputs["generation_status"] = "success"
            else:
                outputs["generation_status"] = f"failed: {result.stderr}"
        else:
            outputs["generation_status"] = "render script not found"

    except Exception as e:
        outputs["generation_status"] = f"error: {e}"

    return outputs


def run_single_test(test_name: str, test_data: dict[str, str]) -> dict[str, Any]:
    """Run verification on a single test model."""
    model_file = Path(test_data["model_path"])
    print(f"üìù Testing: {test_name}")

    # Create output directory for this model
    model_output_dir = Path(__file__).parent / "test_outputs" / test_name

    # Run the verification
    result = server.verify_cad_query(test_data["model_path"], test_data["criteria"])

    # Generate visual outputs
    print(f"   Generating visual outputs...")
    visual_outputs = generate_visual_outputs(model_file, model_output_dir)

    # Check if result matches expectation
    actual_status = result["status"]
    expected_status = test_data["expected"]

    is_correct = actual_status == expected_status

    print(f"   Expected: {expected_status}")
    print(f"   Actual:   {actual_status}")
    print(f"   Result:   {'‚úÖ PASS' if is_correct else '‚ùå FAIL'}")
    print(f"   Visual:   {visual_outputs.get('generation_status', 'unknown')}")

    if not is_correct:
        print(f"   Details:  {result.get('details', 'N/A')}")

    return {
        "test_name": test_name,
        "expected": expected_status,
        "actual": actual_status,
        "correct": is_correct,
        "result": result,
        "visual_outputs": visual_outputs,
        "output_dir": str(model_output_dir),
    }


def run_evaluation() -> dict[str, Any]:
    """Run evaluation on all test models."""
    print("üöÄ CAD Verification Evaluation Harness")
    print("=" * 50)

    # Load test cases
    try:
        test_cases = load_test_cases()
        print(f"üìã Loaded {len(test_cases)} test cases")
    except Exception as e:
        print(f"‚ùå Error loading test cases: {e}")
        return {"error": str(e)}

    # Run tests
    results = []
    correct_count = 0
    total_count = 0
    true_positives = 0
    true_negatives = 0
    false_positives = 0
    false_negatives = 0

    print("\nüß™ Running Tests:")
    print("-" * 30)

    for test_name, test_data in test_cases.items():
        test_result = run_single_test(test_name, test_data)
        results.append(test_result)

        if test_result["correct"]:
            correct_count += 1
            
        # Calculate confusion matrix values
        expected = test_result["expected"]
        actual = test_result["actual"]
        
        if expected == "PASS" and actual == "PASS":
            true_positives += 1
        elif expected == "FAIL" and actual == "FAIL":
            true_negatives += 1
        elif expected == "FAIL" and actual == "PASS":
            false_positives += 1
        elif expected == "PASS" and actual == "FAIL":
            false_negatives += 1
            
        total_count += 1
        print()  # Empty line for readability

    # Calculate metrics
    accuracy = (correct_count / total_count * 100) if total_count > 0 else 0
    
    # Calculate precision, recall, F1
    precision = (true_positives / (true_positives + false_positives)) if (true_positives + false_positives) > 0 else 0
    recall = (true_positives / (true_positives + false_negatives)) if (true_positives + false_negatives) > 0 else 0
    f1_score = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0

    # Print summary
    print("üìä Evaluation Summary:")
    print("=" * 30)
    print(f"Total Tests:    {total_count}")
    print(f"Correct:        {correct_count}")
    print(f"Incorrect:      {total_count - correct_count}")
    print(f"Accuracy:       {accuracy:.1f}%")
    print(f"Precision:      {precision:.3f}")
    print(f"Recall:         {recall:.3f}")
    print(f"F1 Score:       {f1_score:.3f}")
    print(f"Visual Outputs: test_outputs/ directory")
    
    # Print confusion matrix
    print("\nüìã Confusion Matrix:")
    print("=" * 20)
    print(f"                Predicted")
    print(f"              PASS  FAIL")
    print(f"Actual PASS   {true_positives:4d}  {false_negatives:4d}")
    print(f"       FAIL   {false_positives:4d}  {true_negatives:4d}")

    if accuracy == 100:
        print("üéâ Perfect score! All tests passed.")
    elif accuracy >= 80:
        print("‚úÖ Good performance, minor issues to address.")
    else:
        print("‚ö†Ô∏è  Significant issues detected, verification logic needs improvement.")

    return {
        "total_tests": total_count,
        "correct": correct_count,
        "accuracy": accuracy,
        "precision": precision,
        "recall": recall,
        "f1_score": f1_score,
        "confusion_matrix": {
            "true_positives": true_positives,
            "true_negatives": true_negatives,
            "false_positives": false_positives,
            "false_negatives": false_negatives
        },
        "results": results,
    }


def main():
    """Main function."""
    try:
        evaluation_results = run_evaluation()

        # Save detailed results
        results_file = Path(__file__).parent / "evaluation_results.json"
        with open(results_file, "w") as f:
            json.dump(evaluation_results, f, indent=2)

        print(f"\nüìÑ Detailed results saved to: {results_file}")

        # Exit with appropriate code
        if evaluation_results.get("accuracy", 0) == 100:
            sys.exit(0)
        else:
            sys.exit(1)

    except Exception as e:
        print(f"‚ùå Evaluation failed: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
